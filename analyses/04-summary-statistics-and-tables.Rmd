---
title: "Summary statistics and tables"
author: "Corresponding author"
date: "2023-30-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggnewscale) # Enables multiple colour- and fill-scales
library(latex2exp)
library(patchwork)
library(xtable)
source("00-functions-and-global-definitions.R")

plot_title_size <- 7
axis_title_size <- 7
axis_ticks_size <- 7
legend_title_size <- 7
legend_text_size <- 7
```

```{r Reading data}
dd_GT <- readRDS(file.path("..", "data", "00_dd_GT_all_loci.rds"))
dd_GT_2nd <- readRDS(file.path("..", "data", "00_dd_GT_2nd_all_loci.rds"))
true_profiles_2nd <- readRDS(file.path("..", "data", "00_true_profiles_2nd.rds"))
dd_GT_both <- bind_rows(dd_GT[,1:26], dd_GT_2nd)

dd_GT_article <- readRDS(file.path("..", "data", "00_dd_GT.rds"))
dd_GT_article <- dd_GT_article |> 
  mutate(Genotype_EQC = threshold_rule(.data))
dd_GT_2nd_article <- readRDS(file.path("..", "data", "00_dd_GT_2nd.rds"))
dd_GT_2nd_article <- dd_GT_2nd_article |> 
  mutate(Genotype_EQC = threshold_rule(.data))

dd_GT_article_both <- bind_rows(dd_GT_article, dd_GT_2nd_article)
dd_MTK <- readRDS(file.path("..", "data", "00_dd_MTK.rds"))
```


# Summary statistics
## Number of NCs and WCs in scatterplot for 31.25pg
```{r}
dd_GT_31.25 <- dd_GT |>
  filter(Dilution == 31.25, !(Target.ID %in% c("rs459920", "rs7251928", "rs7722456"))) |> 
  mutate(Genotype_EQC = threshold_rule(.data))

summarise(.data = dd_GT_31.25,
          "Total SNPs" = n(),
          "HSG NC" = sum(Genotype == "NC"),
          "HSG WC" = sum(Genotype != "NC" & Genotype != Genotype_true),
          "EQC NC" = sum(Genotype_EQC == "NC"),
          "EQC WC" = sum(Genotype_EQC != "NC" & Genotype_EQC != Genotype_true)) |> t()
```

```{r}
m_fit <- fit_symmetry_model2(df = dd_GT_31.25, f = sqrt, intercept = T, b_int = c(0, 1,-2))
dd_GT_31.25 <- predict_prob_threshold(dd_GT_31.25, m_fit)
fit_WC <- dd_GT_31.25 |> filter(Genotype_pred != Genotype_true)
fit_CC <- dd_GT_31.25 |> filter(Genotype_pred == Genotype_true)
p_WC <- fit_WC$p_max[order(fit_WC$p_max)]
p_CC <- fit_CC$p_max
lapply(p_WC, function(x) {
  n_WC <- sum(p_WC >= x)
  n_NC <- sum(p_WC < x) + sum(p_CC < x)
  return(list("NC" = n_NC, "WC" = n_WC, "q" = x))
}) |> bind_rows()
sum(p_WC >= 0.9767)
sum(p_WC < 0.9767) + sum(p_CC < 0.9767)
```


## True genotypes for the first dilution series
The first dilution series was measured in four independent runs.
In Run A, B, and D, all six dilutions from 1000pg halved until 31.25pg were measure, but Run C ends at 62.5pg, because the library was used up.
Furthermore, for Dilution 62.5pg in Run C, only one of the individuals were measured, and for Dilution 31.25pg in Run D, only two of the individuals were measured.
Hence, we have $108$ attempts to measure a profile, and $21$ runs/dilutions containing data from all five individuals.

Below we see that 16 of the runs and dilutions agree on the `Genotype` provided by the HID SNP Genotyper plug-in (HSG).
That is, in the six runs/dilutions from the first five comparisons below, the HSG didn't make any no-calls (NCs), and they agree on each measured genotype, but for the next eight comparisons, the HSG made a few NCs, and if we ignore these, the runs/dilutions also agree on each measured genotype.

Then Dilution 250pg and 125pg from Run C each have a single genotype that don't match the previous 14 agreeing measurements, and likewise the Dilution 62.5pg and 31.25pg from Run B have two and four non-matching genotypes, respectively.

Five of the lower dilutions in Run A, C, and D have missing genotypes (i.e. not no-calls, but genotypes that simply weren't measured).
For two of these, the remaining genotypes agree with the first five comparisons when we ignore their NCs.
The one of these (Dilution 62.5pg in Run C) also agreed with the first five comparisons when we ignore their NCs, but this sample only has data from one individual.
For the last two, both had three non-matching genotypes.

Overall, this gives six fully matching measurement (i.e. five pairwise comparisons give us six different runs/dilutions) and ten matching measurements for all five individuals when no-calls are ignored, and a matching measurement for a single individual when no-calls are ignored.
We can therefore say, that 16 out of 21 runs/dilutions agreed on the actually called genotypes for all five individuals.
```{r}
genotype_HSG <- dd_GT |> mutate(run_dil = interaction(Run, Dilution))
genotype_HSG <- split(genotype_HSG, as.factor(genotype_HSG$run_dil))
genotype_HSG <- genotype_HSG[-3] # Run C don't have a dilution 31.25


# In the following six 'Runs' x 'Dilutions', all SNPs were predicted by the HSG
# (i.e. it made 0 no-calls, but actual genotype predictions for all markers)
names(genotype_HSG)[sapply(genotype_HSG, function(x) {!any(x$Genotype == "NC")})]
# All of these full profiles agree for each individual
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$C.1000$Genotype)
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$D.1000$Genotype)
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$B.500$Genotype)
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$D.500$Genotype)
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$D.250$Genotype)
# Since these full profiles agree and were measured with high DNA quantities,
# we will assume that they are the true profiles.

# We further have twelve profiles where all markers were measured, but some were no-calls
names(genotype_HSG)[sapply(genotype_HSG, function(x) {nrow(x) == nrow(genotype_HSG$A.1000) & any(x$Genotype == "NC")})]
# Eight of these profiles agree when ignoring no-calls:
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$B.1000$Genotype & genotype_HSG$B.1000$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$A.500$Genotype & genotype_HSG$A.500$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$C.500$Genotype & genotype_HSG$C.500$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$A.250$Genotype & genotype_HSG$A.250$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$B.250$Genotype & genotype_HSG$B.250$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$B.125$Genotype & genotype_HSG$B.125$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$D.125$Genotype & genotype_HSG$D.125$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$D.62.5$Genotype & genotype_HSG$D.62.5$Genotype != "NC")
# And four of them have disagreements (according to the assumed true profiles)
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$C.250$Genotype & genotype_HSG$C.250$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$C.125$Genotype & genotype_HSG$C.125$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$B.62.5$Genotype & genotype_HSG$B.62.5$Genotype != "NC")
sum(genotype_HSG$A.1000$Genotype != genotype_HSG$B.31.25$Genotype & genotype_HSG$B.31.25$Genotype != "NC")

# The last five 'Runs' x 'Dilutions' do not contain the full 5*165 = 825 SNP measurements,
# either because of limited DNA material for some individuals (which then weren't examined),
# or because of NAs
names(genotype_HSG)[sapply(genotype_HSG, function(x) {nrow(x) != nrow(genotype_HSG$A.1000) & any(x$Genotype == "NC")})]
# When ignoring no-calls, three of these sets of partial profiles do agree
dd_combined <- genotype_HSG$A.125[,c("ID", "Target.ID", "Genotype")] |> inner_join(genotype_HSG$A.1000[,c("ID", "Target.ID", "Genotype")], by = c("ID", "Target.ID"))
sum(dd_combined$Genotype.x != dd_combined$Genotype.y & dd_combined$Genotype.x != "NC")
dd_combined <- genotype_HSG$A.62.5[,c("ID", "Target.ID", "Genotype")] |> inner_join(genotype_HSG$A.1000[,c("ID", "Target.ID", "Genotype")], by = c("ID", "Target.ID"))
sum(dd_combined$Genotype.x != dd_combined$Genotype.y & dd_combined$Genotype.x != "NC")
# Dilution 62.5pg in Run C only contains a single individual
dd_combined <- genotype_HSG$C.62.5[,c("ID", "Target.ID", "Genotype")] |> inner_join(genotype_HSG$A.1000[,c("ID", "Target.ID", "Genotype")], by = c("ID", "Target.ID"))
sum(dd_combined$Genotype.x != dd_combined$Genotype.y & dd_combined$Genotype.x != "NC")
# And two of the sets with partial profiles have disagreements
dd_combined <- genotype_HSG$A.31.25[,c("ID", "Target.ID", "Genotype")] |> inner_join(genotype_HSG$A.1000[,c("ID", "Target.ID", "Genotype")], by = c("ID", "Target.ID"))
sum(dd_combined$Genotype.x != dd_combined$Genotype.y & dd_combined$Genotype.x != "NC" & dd_combined$Genotype.y != "NC")
# Dilution 31.25pg in Run D only contains two individuals
dd_combined <- genotype_HSG$D.31.25[,c("ID", "Target.ID", "Genotype")] |> inner_join(genotype_HSG$A.1000[,c("ID", "Target.ID", "Genotype")], by = c("ID", "Target.ID"))
sum(dd_combined$Genotype.x != dd_combined$Genotype.y & dd_combined$Genotype.x != "NC" & dd_combined$Genotype.y != "NC")


# Does any of the high concentration subdatasets that don't match A.1000 completely
# have a complete match between each other:
sum(genotype_HSG$B.1000$Genotype != genotype_HSG$C.250$Genotype & genotype_HSG$B.1000$Genotype != "NC" & genotype_HSG$C.250$Genotype != "NC")
sum(genotype_HSG$C.500$Genotype != genotype_HSG$C.250$Genotype & genotype_HSG$C.500$Genotype != "NC" & genotype_HSG$C.250$Genotype != "NC")
sum(genotype_HSG$A.500$Genotype != genotype_HSG$C.250$Genotype & genotype_HSG$A.500$Genotype != "NC" & genotype_HSG$C.250$Genotype != "NC")
sum(genotype_HSG$B.250$Genotype != genotype_HSG$C.250$Genotype & genotype_HSG$B.250$Genotype != "NC" & genotype_HSG$C.250$Genotype != "NC")
```


# Data summaries
Number of dilutions and runs where all 162 relevant markers were observed for all five individuals including no-calls (eighteen times)
```{r}
obs_geno <- sapply(genotype_HSG, function(x){
  x |> filter(!(Target.ID %in% c("rs7722456", "rs459920", "rs7251928"))) |> nrow()
})
sum(obs_geno == 5*162)
```
Number of dilutions and runs where all 162 relevant markers were observed for all five individuals excluding no-calls (six times).
```{r}
obs_geno <- sapply(genotype_HSG, function(x){
  x |> filter(!(Target.ID %in% c("rs7722456", "rs459920", "rs7251928")), Genotype != "NC", !is.na(Genotype)) |> nrow()
})
sum(obs_geno == 5*162)
```
Previously, we found complete agreement between dilution 1000 from run A, C, and D, which also matched dilution 500 from run B and D as well as dilution 250 from run D, i.e., we found six runs/dilutions that fully agreed on the Genotype-variable from the HID SNP Genotyper.
We see that these six fully agreeing runs/dilutions are also the six subdatasets where all 5*162=810 SNPs are observed with no NCs:
```{r}
(fully_agreeing <- c("A.1000", "C.1000", "D.1000", "B.500", "D.500", "D.250"))
(full_data <- names(obs_geno[obs_geno == 5*162]))
sum(fully_agreeing %in% full_data)
```



# The HSG's NCs, WCs, Call rates, and accuracies for used data
```{r}
(dd_hsg <- dd_MTK |> 
  mutate(Low_high = (Run == "a")*1) |> 
  group_by(Dilution, Low_high) |> 
  summarise("NCs" = sum(Genotype == "NC"),
            "WCs" = sum(Genotype != "NC" & Genotype != Genotype_true),
            "Tot_calls" = n(),
#            "p_NC" = 100*round(NCs/Tot_calls, digits=4),
#            "p_WC" = 100*round(WCs/Tot_calls, digits=4),
            "Call rate" = 100*round((Tot_calls-NCs)/Tot_calls, digits=4),
            "Accuracy" = 100*round((Tot_calls-NCs-WCs)/(Tot_calls-NCs), digits=4)) |> 
  arrange(Low_high, desc(Dilution)) |> select(-Low_high))
```

# SMLR's NCs, WCs, Call rates, and accuracies for used data
```{r}
m_fit <- dd_GT_article_both |> filter(Dilution %in% c(50, 25)) |>
  fit_symmetry_model2(f = sqrt, intercept = T, b_int = c(0, 1,-2))
q <- 0.84

dd_pred <- predict_prob_threshold(dd_MTK, m_fit) |> 
  mutate(WC_l = Genotype_pred != Genotype_true & Genotype_pred != no_call, # logical indicators of WCs
         uWC_l = MTK_pred_u != Genotype_true & MTK_pred_u != no_call,
         mWC_l = MTK_pred != Genotype_true & MTK_pred != no_call) |> 
  split(~Dilution)
```

```{r}
dd_smlr_fixed_q <- dd_pred |> 
  lapply(function(d) {
    p <- d |> arrange(p_max) |> select(p_max, WC_l)
    # hsg_nc <- dd_hsg |> ungroup() |> filter(Dilution == unique(d$Dilution)) |> select(NCs) |> unlist()
    p_q <- p[p$p_max >= q,]
    smlr_nc <- nrow(p[p$p_max < q,])
    
    data.frame("Low_high" = ("a" %in% d$Run)*1, "NCs" = smlr_nc, "WCs" = sum(p_q$WC_l), "Tot_calls" = nrow(p)) |> 
      mutate("Call rate" = 100*round((Tot_calls-NCs)/Tot_calls, digits=4),
             "Accuracy" = 100*round((Tot_calls-NCs-WCs)/(Tot_calls-NCs), digits=4))
  }) |> bind_rows(.id = "Dilution") |> 
  mutate(Dilution = as.numeric(Dilution)) |> 
  arrange(Low_high, desc(Dilution)) |> select(-Low_high)

rownames(dd_smlr_fixed_q) <- NULL
dd_smlr_fixed_q
```

# Improvements for q=0.84
```{r}
select(dd_hsg, Dilution, Tot_calls, NC_hsg = NCs, WC_hsg = WCs) |> 
  left_join(select(dd_smlr_fixed_q, Dilution, Tot_calls, NC_smlr = NCs, WC_smlr = WCs)) |> 
  mutate(NC_improve = 100*round((NC_hsg-NC_smlr)/NC_hsg, digits=4),
         WC_improve = 100*round((WC_hsg-WC_smlr)/WC_hsg, digits=4)) |> 
  select(Dilution, Tot_calls, NC_hsg, NC_smlr, NC_improve, WC_hsg, WC_smlr, WC_improve)
```

# Improvements for aligned q
NOTE:
- To make fair and conservative comparisons of no-calls (or call rates), we want SMLR to have at least the same accuracy as HSG, i.e. the same number of WCs or fewer (sometimes WCs come in multiples for the same q-value, so we may not always be able to hit the same accuracies, but then we will hit an even higher accuracy, which implies having turned more observations into NCs, which favours the HSG in the call rate comparison).
- To make fair and conservative comparisons of wrong calls (or accuracies), we want SMLR to have at least the same call rate as HSG, i.e. the same number of NCs or fewer (sometimes NCs come in multiples for the same q-value, so we may not always be able to hit the same call rates, but then we will hit an even higher call rate, which implies having turned fewer observations into NCs, which again implies a lower accuracy, thus favouring the HSG in the comparison of accuracy).
```{r Table no-calls and wrong calls}
dd_smlr_nc <- dd_pred |> 
  lapply(function(d) {
    # HSG
    hsg_wc <- dd_hsg |> ungroup() |> filter(Dilution == unique(d$Dilution)) |> select(WCs) |> unlist()
    hsg_nc <- dd_hsg |> ungroup() |> filter(Dilution == unique(d$Dilution)) |> select(NCs) |> unlist()
    
    # SMLR
    # We now arrange the probabilities for the WCs in decreasing order, such that
    # the bottom ones are turned into NCs first. Then we make a numeration from
    # top to bottom (WC_cum), such that the WC to be removed last has number 1 (the top one).
    # Then, if the HSG made x WCs, we should as minimum 'NC-convert' the WCs for
    # SMLR with WC_cum < x, and maybe more if the WCs with WC_cum >= x has the
    # same p_max as the one with WC_cum = x-1.
    # Recall that we turn a SNP into an NC if its p_max falls short of q,
    # i.e. we accept the prediction if p_max >= q and turn into NC if p_max < q.
    # This means that we should use the p_max for WC_cum = x in order to turn
    # the WCs at WC_cum < x into NCs, but there is a twist: if p_max has the
    # same value for WC_cum = x and WC_cum = x-1, we have to search at WC_cum > x
    # for an even higher p_max (i.e. then we can't make SMLR yield exactly the
    # same number of WCs as the HSG, so we need to make it produce fewer WCs).
    p_cr <- d |> filter(WC_l) |> select(p_max, WC_l) |> arrange(desc(p_max)) |> mutate(WC_cum = cumsum(WC_l))
    # We only need to search for a WC-alignment threshold if the raw SMLR (q=0)
    # gave the same or fewer WCs than the HSG:
    if (nrow(p_cr) > hsg_wc) {
      q_same_wc <- p_cr |> filter(WC_cum <= hsg_wc) |> 
        filter(WC_cum == max(WC_cum)) |> select(p_max) |> unlist()
      # If there are more than one WC with p_max equal to q_same_wc, and this WC has a
      # higher value at WC_cum, then we should select a higher p_max in order to make
      # the SMLR to have at least the same accuracy as HSG.
      if (any(filter(p_cr, WC_cum > hsg_wc)$p_max == q_same_wc)) {
        q_same_wc <- p_cr |> filter(p_max > q_same_wc) |> 
          filter(WC_cum == max(WC_cum)) |> select(p_max) |> unlist()
      }
      smlr_nc <- sum(d$p_max < q_same_wc)
    } else { # if the raw SMLR (q=0) gave the same or fewer WCs than the HSG
      q_same_wc <- 0
      smlr_nc <- 0
    }
    smlr_wc <- d |> filter(p_max >= q_same_wc, WC_l) |> nrow()
    
    # MTK (with prior derived from data)
    p_cr_m <- d |> filter(mWC_l) |> select(MTK_pmax, mWC_l) |> arrange(desc(MTK_pmax)) |> mutate(mWC_cum = cumsum(mWC_l))
    if (nrow(p_cr_m) > hsg_wc) {
      q_same_wc_m <- p_cr_m |> filter(mWC_cum <= hsg_wc) |> 
        filter(mWC_cum == max(mWC_cum)) |> select(MTK_pmax) |> unlist()
      if (any(filter(p_cr_m, mWC_cum > hsg_wc)$MTK_pmax == q_same_wc_m)) {
        q_same_wc_m <- p_cr_m |> filter(MTK_pmax > q_same_wc_m) |> 
          filter(mWC_cum == max(mWC_cum)) |> select(MTK_pmax) |> unlist()
      }
      MTK_m_nc <- sum(d$MTK_pmax < q_same_wc_m)
    } else {
      q_same_wc_m <- 0
      MTK_m_nc <- 0
    }
    MTK_m_wc <- d |> filter(MTK_pmax >= q_same_wc_m, mWC_l) |> nrow()
    
    # MTK (with biallelic uniform prior)
    p_cr_u <- d |> filter(uWC_l) |> select(MTK_pmax_u, uWC_l) |> arrange(desc(MTK_pmax_u)) |> mutate(uWC_cum = cumsum(uWC_l))
    if (nrow(p_cr_u) > hsg_wc) {
      q_same_wc_u <- p_cr_u |> filter(uWC_cum <= hsg_wc) |> 
        filter(uWC_cum == max(uWC_cum)) |> select(MTK_pmax_u) |> unlist()
      if (any(filter(p_cr_u, uWC_cum > hsg_wc)$MTK_pmax_u == q_same_wc_u)) {
        q_same_wc_u <- p_cr_u |> filter(MTK_pmax_u > q_same_wc_u) |> 
          filter(uWC_cum == max(uWC_cum)) |> select(MTK_pmax_u) |> unlist()
      }
      MTK_u_nc <- sum(d$MTK_pmax_u < q_same_wc_u)
    } else {
      q_same_wc_u <- 0
      MTK_u_nc <- 0
    }
    MTK_u_wc <- d |> filter(MTK_pmax_u >= q_same_wc_u, uWC_l) |> nrow()
    
    data.frame("Low_high" = ("a" %in% d$Run)*1,
               "Tot_calls" = nrow(d),
               "HSG_NC" = hsg_nc,
               "HSG_WC" = hsg_wc,
               "q_same_WC" = q_same_wc,
               "SMLR_call_rate" = 100*(nrow(d)-smlr_nc)/nrow(d),
               "SMLR_NC" = smlr_nc,
               "Improve_NC" = case_when(hsg_nc == 0 & smlr_nc == 0 ~ 0,
                                        hsg_nc == 0 & smlr_nc > 0 ~ Inf,
                                        .default = 100*(hsg_nc-smlr_nc)/hsg_nc),
               "q_same_WC_m" = q_same_wc_m,
               "MTK_m_call_rate" = 100*(nrow(d)-MTK_m_nc)/nrow(d),
               "MTK_m_NC" = MTK_m_nc,
               "q_same_WC_u" = q_same_wc_u,
               "MTK_u_call_rate" = 100*(nrow(d)-MTK_u_nc)/nrow(d),
               "MTK_u_NC" = MTK_u_nc)
  }) |> bind_rows(.id = "Dilution") |> 
  mutate(Dilution = as.numeric(Dilution)) |> 
  arrange(Low_high, desc(Dilution)) |> select(-Low_high)
rownames(dd_smlr_nc) <- NULL

dd_smlr_wc <- dd_pred |> 
  lapply(function(d) {
    # HSG
    hsg_wc <- dd_hsg |> ungroup() |> filter(Dilution == unique(d$Dilution)) |> select(WCs) |> unlist()
    hsg_nc <- dd_hsg |> ungroup() |> filter(Dilution == unique(d$Dilution)) |> select(NCs) |> unlist()
    
    # SMLR
    p_ac <- d |> arrange(p_max) |> select(p_max, WC_l) |> rownames_to_column("idx") |> mutate(idx = as.integer(idx))
    q_same_nc <- p_ac |> filter(idx > hsg_nc) |>
      filter(idx == min(idx)) |> select(p_max) |> unlist()
    # Recall that we turn a SNP into an NC if its p_max falls short of q, i.e., we
    # accept the prediction if p_max>=q and turn into NC if p_max<q.
    # Since we want the same number of NCs as the HSG or fewer, then we only need
    # to order according to p_max and extract p_max from the rownumber equal to hsg_nc.
    # There will likely be more observations with this p_max on either side of this row,
    # but since all of these are accepted as valid predictions (since p_max>=q_same_nc),
    # then the p_max of this row is exactly the q_same_nc we are looking for, regardless
    # of whether there are other observations with the same p_max and how these are arranged.
    smlr_wc <- d |> filter(p_max >= q_same_nc, WC_l) |> nrow()
    smlr_nc <- d |> filter(p_max < q_same_nc) |> nrow()
    
    # MTK (with prior derived from data)
    p_ac_m <- d |> arrange(MTK_pmax) |> select(MTK_pmax, mWC_l) |> rownames_to_column("idx") |> mutate(idx = as.integer(idx))
    q_same_nc_m <- p_ac_m |> filter(idx > hsg_nc) |>
      filter(idx == min(idx)) |> select(MTK_pmax) |> unlist()
    MTK_m_wc <- d |> filter(MTK_pmax >= q_same_nc_m, mWC_l) |> nrow()
    MTK_m_nc <- d |> filter(MTK_pmax < q_same_nc_m) |> nrow()
    
    # MTK (with biallelic uniform prior)
    p_ac_u <- d |> arrange(MTK_pmax_u) |> select(MTK_pmax_u, uWC_l) |> rownames_to_column("idx") |> mutate(idx = as.integer(idx))
    q_same_nc_u <- p_ac_u |> filter(idx > hsg_nc) |>
      filter(idx == min(idx)) |> select(MTK_pmax_u) |> unlist()
    MTK_u_wc <- d |> filter(MTK_pmax_u >= q_same_nc_u, uWC_l) |> nrow()
    MTK_u_nc <- d |> filter(MTK_pmax_u < q_same_nc_u) |> nrow()
    
    data.frame("Low_high" = ("a" %in% d$Run)*1,
               "Tot_calls" = nrow(d),
               "HSG_NC" = hsg_nc,
               "HSG_WC" = hsg_wc,
               "q_same_NC" = q_same_nc,
               "SMLR_accuracy" = 100*(nrow(d)-smlr_nc-smlr_wc)/(nrow(d)-smlr_nc),
               "SMLR_WC" = smlr_wc,
               "Improve_WC" = case_when(hsg_wc == 0 & smlr_wc == 0 ~ 0,
                                        hsg_wc == 0 & smlr_wc > 0 ~ Inf,
                                        .default = 100*(hsg_wc-smlr_wc)/hsg_wc),
               "q_same_NC_m" = q_same_nc_m,
               "MTK_m_accuracy" = 100*(nrow(d)-MTK_m_nc-MTK_m_wc)/(nrow(d)-MTK_m_nc),
               "MTK_m_WC" = MTK_m_wc,
               "q_same_NC_u" = q_same_nc_u,
               "MTK_u_accuracy" = 100*(nrow(d)-MTK_u_nc-MTK_u_wc)/(nrow(d)-MTK_u_nc),
               "MTK_u_WC" = MTK_u_wc)
  }) |> bind_rows(.id = "Dilution") |> 
  mutate(Dilution = as.numeric(Dilution)) |> 
  arrange(Low_high, desc(Dilution)) |> select(-Low_high)
rownames(dd_smlr_wc) <- NULL

dd_smlr_nc_wc <- cbind(dd_smlr_nc, dd_smlr_wc[,5:14])

ncwc.table <- t(dd_smlr_nc_wc) |> as.data.frame() |> rownames_to_column("V0")
# colnames(ncwc.table) <- c("DNA quantity (pg)", unlist(ncwc.table[1,-1]))
# ncwc.table <- ncwc.table |> filter(`DNA quantity (pg)` != "Dilution")
# ncwc.table$`DNA quantity (pg)` <- c("Number of SNPs", "HSG no-calls", "HSG wrong calls", "q aligning WCs", "SMLR call rate", "SMLR no-calls", "NC-reduction (%)", "q aligning NCs", "SMLR accuracy", "SMLR wrong calls", "WC-reduction (%)")
# ncwc.table <- column_to_rownames(ncwc.table, "DNA quantity (pg)")
ncwc.table$V0 <- c("DNA quantity (pg)", "Number of SNPs", "HSG no-calls", "HSG wrong calls", "q aligning WCs", "SMLR call rate (%)", "SMLR no-calls", "NC-reduction", "q aligning WCs", "MTK call rate (%)", "MTK no-calls", "q aligning WCs", "MTKU call rate (%)", "MTKU no-calls", "q aligning NCs", "SMLR accuracy (%)", "SMLR wrong calls", "WC-reduction", "q aligning NCs", "MTK accuracy (%)", "MTK wrong calls", "q aligning NCs", "MTKU accuracy (%)", "MTKU wrong calls") # The (%) after the NC- and WC-reductions are easier to insert in the subsequent LaTeX-refinements

# Rearranging table rows to first list NCs of methods in order of
# HSG, MTK, MTK uniform, SMLR
# and then list WCs with same method order
ncwc.table <- ncwc.table[c(1:4, 9:14, 5:8, 19:24, 15:18),]
row.names(ncwc.table) <- 1:24

# Handling of q close to 1: due to limited horizontal space and aesthetics, we
# display at max four digits for q. It is also rather optimistic to hope for a
# precision higher than this, i.e., that a probability estimate of,
# e.g., 0.99999976 can be trusted as a good estimate.
# As we see there are no q-values that are exactly 1 (which would also have
# implied a call rate of 0 and an undefined accuracy):
any(ncwc.table[c(5, 8, 11, 15, 18, 21), -1] == 1)
# But there are q-values with more nines than our display limit:
sum(ncwc.table[c(5, 8, 11, 15, 18, 21), -1] >= 0.9999)
# To avoid that these get displayed as 1.0000, we overwrite with 0.9999:
ncwc.table[c(5, 8, 11, 15, 18, 21), -1][ncwc.table[c(5, 8, 11, 15, 18, 21), -1] >= 0.9999] <- 0.9999

# Digit matrix (initialised with zeros)
d_mat <- matrix(0, nrow(ncwc.table), ncol(ncwc.table))
# The first column should continue to be zeros, since it is inhabited with strings.
# The following ten rows (with integers) should continue to have zeros in d_mat:
# "DNA quantity (pg)", "Number of SNPs", "HSG no-calls", "HSG wrong calls",
# "MTK no-calls", "MTK wrong calls", "MTKU no-calls", "MTKU wrong calls",
# "SMLR no-calls", "SMLR wrong calls", i.e. the row numbers 1, 2, 3, 4, 7, 10, 13, 17, 20, 23.
# The first row with DNA quantities should be zeros for the integer values, and
# only have the necessary digits for the DNA amounts 62.5, 31.25, 12.5, 6.25
d_mat[1, ncwc.table[1,] %in% c(62.5, 12.5)] <- 1
d_mat[1, ncwc.table[1,] %in% c(31.25, 6.25)] <- 2
# The ten rows with q-values (row 5, 8, 11, 15, 18, 21) should have four digits,
# except when the q-value is 0, in which case there should be zero digits, but
# this deviation from the standard pattern is handled later:
d_mat[c(5, 8, 11, 15, 18, 21), 2:ncol(ncwc.table)] <- 4
# The five rows "MTK call rate", "MTKU call rate", "SMLR call rate",
# "NC-reductions", "WC-reductions" should have one digit (i.e. row 6, 9, 12, 14, 24).
# Deviations from this are handled later.
d_mat[c(6, 9, 12, 14, 24), 2:ncol(ncwc.table)] <- 1
# Row number 16, 19, 22 with accuracies should have three digits (deviations are handled later).
d_mat[c(16, 19, 22), 2:ncol(ncwc.table)] <- 3

# Deviations from the standard digit pattern are where a zero value or a value
# of 100 percentage points (or a 1) are prettier with zero digits:
dev_mat <- matrix(F, nrow(ncwc.table), ncol(ncwc.table))
# q-thresholds to have zero digits:
dev_mat[c(5, 8, 11, 15, 18, 21),] <- ncwc.table[c(5, 8, 11, 15, 18, 21),] == 0
# Accuracies, call rates, and NC-reductions to have zero digits:
dev_mat[c(6, 9, 12, 14, 16, 19, 22, 24),] <- ncwc.table[c(6, 9, 12, 14, 16, 19, 22, 24),] == 0 | ncwc.table[c(6, 9, 12, 14, 16, 19, 22, 24),] == 100
d_mat[dev_mat] <- 0
d_mat <- cbind(0, cbind(0, d_mat))
ncwc.table <- cbind(NA, ncwc.table)

library(xtable)
ncwc.table <- xtable(ncwc.table,
                     caption = "The SMLR modelâ€™s genotyping improvements for all examined DNA quantities.",
                     # caption = "The SMLR model's genotyping improvements relative to the HID SNP Genotyper Plugin (HSG).",
                     label = "tab:NC_WC",
                     digits = d_mat)

print(ncwc.table, file = file.path("..", "article-FSIGEN", "xtable1.tex"),
      floating.environment = "table*",
      table.placement = NULL,
      caption.placement = "top",
      format.args = list(big.mark = ",", decimal.mark = "."),
      include.rownames = F, include.colnames = F, booktabs = T)
```

Additional table LaTeX-refinements
```{r Table LaTeX-refinements}
tab1 <- read_lines(file.path("..", "article-FSIGEN", "xtable1.tex"))

tab1 <- c(tab1[3:6],
          "\\vspace{1mm}",
          "\\begin{adjustbox}{max width=\\textwidth}",
          "\\begin{threeparttable}",
          tab1[7], paste0(tab1[8], "[2pt]"),
          "\\null & \\null & \\multicolumn{6}{c}{First series:} & \\multicolumn{4}{c}{Second series:} \\\\",
          "\\null & \\null & \\multicolumn{6}{c}{5 individuals, 4 examinations} & \\multicolumn{4}{c}{18 individuals, 1 examination} \\\\",
          "\\cmidrule(lr){3-8} \\cmidrule(lr){9-12}",
          sub(" & DNA quantity (pg)", "\\multicolumn{2}{l}{DNA quantity (\\si{\\pg})}", tab1[10], fixed = TRUE),
          sub("   & Number of SNPs", "\\multicolumn{2}{l}{Observed SNPs ($s_1\\!+\\!s_2\\!>\\!0$)}", tab1[11], fixed = TRUE),
          "\\midrule[2pt]",
          sub("   & HSG no-calls", "\\multirow{2}{*}{HSG\\tnote{$*$}} & No-calls", tab1[12], fixed = TRUE),
          sub("HSG wrong calls", "Wrong calls", tab1[13], fixed = TRUE),
          "\\midrule[2pt]",
          sub("   & q aligning WCs", "\\multirow{4}{*}{SMLR\\tnote{$\\S$}}  & $q$ aligning WCs\\tnote{$\\dagger$}", tab1[20], fixed = TRUE),
          "\\cmidrule(lr){2-12}",
          sub("SMLR call rate", "Call rate", tab1[21], fixed = TRUE),
          sub("SMLR no-calls", "No-calls", tab1[22], fixed = TRUE),
          "\\cmidrule(lr){2-12}",
          sub("NC-reduction", "NC-reduction (\\%)", tab1[23], fixed = TRUE),
          "\\midrule[2pt]",
          sub("   & q aligning NCs", "\\multirow{4}{*}{SMLR\\tnote{$\\S$}} & $q$ aligning NCs\\tnote{$\\ddagger$}", tab1[30], fixed = TRUE),
          "\\cmidrule(lr){2-12}",
          sub("SMLR accuracy", "Accuracy", tab1[31], fixed = TRUE),
          sub("SMLR wrong calls", "Wrong calls", tab1[32], fixed = TRUE),
          "\\cmidrule(lr){2-12}",
          sub("WC-reduction", "WC-reduction (\\%)", tab1[33], fixed = TRUE),
          paste0(tab1[34], "[2pt]"),
          tab1[35],
          "\\begin{tablenotes}",
          "\\item[$*$] Number of no-calls (NCs) and wrong calls (WCs) made by the HID SNP Genotyper Plugin (HSG).",
          "\\item[$\\dagger$] The smallest $q$ that aligns the SMLR model's WCs according to \\eqref{eq:metric_CR}.",
          "\\item[$\\ddagger$] The smallest $q$ that aligns the SMLR model's NCs according to \\eqref{eq:metric_AC}.",
          "\\item[$\\S$] Results are conservative and based on the SMLR model with an intercept fitted to square-root transformed allele signals from examinations of \\SI{25}{\\pg} and \\SI{50}{\\pg} DNA.",
          "\\end{tablenotes}",
          "\\end{threeparttable}",
          "\\end{adjustbox}",
          tab1[36])

write_lines(tab1, file.path("..", "article-FSIGEN", "table1.tex"))
file.remove(file.path("..", "article-FSIGEN", "xtable1.tex"))
```

MTK table LaTeX-refinements
```{r Table with MTK LaTeX-refinements}
# tab1 <- read_lines(file.path("..", "article-FSIGEN", "xtable1.tex"))
# 
# tab1 <- c(tab1[3:6],
#           "\\vspace{1mm}",
#           "\\begin{adjustbox}{max width=\\textwidth}",
#           "\\begin{threeparttable}",
#           tab1[7], paste0(tab1[8], "[2pt]"),
#           "\\null & \\null & \\multicolumn{6}{c}{First series:} & \\multicolumn{4}{c}{Second series:} \\\\",
#           "\\null & \\null & \\multicolumn{6}{c}{5 individuals, 4 examinations} & \\multicolumn{4}{c}{18 individuals, 1 examination} \\\\",
#           "\\cmidrule(lr){3-8} \\cmidrule(lr){9-12}",
#           sub(" & DNA quantity (pg)", "\\multicolumn{2}{l}{DNA quantity (\\si{\\pg})}", tab1[10], fixed = TRUE),
#           # "\\multicolumn{2}{l}{$m$ used by MTKU} & 333 & 167 & 83 & 42 & 21 & 10 & 17 & 8 & 4 & 2 \\\\ ",
#           sub("   & Number of SNPs", "\\multicolumn{2}{l}{Observed SNPs ($s_1\\!+\\!s_2\\!>\\!0$)}", tab1[11], fixed = TRUE),
#           "\\midrule[2pt]",
#           sub("   & HSG no-calls", "\\multirow{2}{*}{HSG} & No-calls", tab1[12], fixed = TRUE),
#           sub("HSG wrong calls", "Wrong calls", tab1[13], fixed = TRUE),
#           "\\midrule[1pt]",
#           "\\midrule[1pt]",
#           # sub("   & q aligning WCs", "\\multirow{3}{*}{MTK}  & $q$ aligning WCs\\tnote{$*$}", tab1[14], fixed = TRUE),
#           # "\\cmidrule(lr){2-12}",
#           # sub("MTK call rate", "Call rate", tab1[15], fixed = TRUE),
#           # sub("MTK no-calls", "No-calls", tab1[16], fixed = TRUE),
#           # "\\midrule[2pt]",
#           sub("   & q aligning WCs", "\\multirow{3}{*}{MTK\\tnote{$*$}}  & $q$ aligning WCs\\tnote{$\\dagger$}", tab1[17], fixed = TRUE),
#           "\\cmidrule(lr){2-12}",
#           sub("MTKU call rate", "Call rate", tab1[18], fixed = TRUE),
#           sub("MTKU no-calls", "No-calls", tab1[19], fixed = TRUE),
#           "\\midrule[2pt]",
#           sub("   & q aligning WCs", "\\multirow{4}{*}{SMLR\\tnote{$\\S$}}  & $q$ aligning WCs\\tnote{$\\dagger$}", tab1[20], fixed = TRUE),
#           "\\cmidrule(lr){2-12}",
#           sub("SMLR call rate", "Call rate", tab1[21], fixed = TRUE),
#           sub("SMLR no-calls", "No-calls", tab1[22], fixed = TRUE),
#           "\\cmidrule(lr){2-12}",
#           sub("NC-reduction", "NC-reduction (\\%)", tab1[23], fixed = TRUE),
#           "\\midrule[1pt]",
#           "\\midrule[1pt]",
#           # sub("   & q aligning NCs", "\\multirow{3}{*}{MTK} & $q$ aligning NCs\\tnote{$\\dagger$}", tab1[24], fixed = TRUE),
#           # "\\cmidrule(lr){2-12}",
#           # sub("MTK accuracy", "Accuracy", tab1[25], fixed = TRUE),
#           # sub("MTK wrong calls", "Wrong calls", tab1[26], fixed = TRUE),
#           # "\\midrule[2pt]",
#           sub("   & q aligning NCs", "\\multirow{3}{*}{MTK\\tnote{$*$}} & $q$ aligning NCs\\tnote{$\\ddagger$}", tab1[27], fixed = TRUE),
#           "\\cmidrule(lr){2-12}",
#           sub("MTKU accuracy", "Accuracy", tab1[28], fixed = TRUE),
#           sub("MTKU wrong calls", "Wrong calls", tab1[29], fixed = TRUE),
#           "\\midrule[2pt]",
#           sub("   & q aligning NCs", "\\multirow{4}{*}{SMLR\\tnote{$\\S$}} & $q$ aligning NCs\\tnote{$\\ddagger$}", tab1[30], fixed = TRUE),
#           "\\cmidrule(lr){2-12}",
#           sub("SMLR accuracy", "Accuracy", tab1[31], fixed = TRUE),
#           sub("SMLR wrong calls", "Wrong calls", tab1[32], fixed = TRUE),
#           "\\cmidrule(lr){2-12}",
#           sub("WC-reduction", "WC-reduction (\\%)", tab1[33], fixed = TRUE),
#           paste0(tab1[34], "[2pt]"),
#           tab1[35],
#           "\\begin{tablenotes}",
#           # "\\item[$*$] The smallest $q$ that aligns the WCs of each of the three methods MTK, MTKU, and SMLR according to \\eqref{eq:metric_CR}.",
#           # "\\item[$\\dagger$] The smallest $q$ that aligns the NCs of each of the three methods MTK, MTKU, and SMLR according to \\eqref{eq:metric_AC}.",
#           "\\item[$*$] Model by Mostad, Tillmar, and Kling (2023) with biallelic uniform prior, $e=0.06$, and $m\\approx \\text{DNA quantity (pg)} \\times 2/6$.",
#           "\\item[$\\dagger$] The smallest $q$ that aligns the WCs according to \\eqref{eq:metric_CR} for each of the two methods MTK and SMLR.",
#           "\\item[$\\ddagger$] The smallest $q$ that aligns the NCs according to \\eqref{eq:metric_AC} for each of the two methods MTK and SMLR.",
#           "\\item[$\\S$] Results are conservative and based on the SMLR model with an intercept fitted to square-root transformed allele signals from examinations of \\SI{25}{\\pg} and \\SI{50}{\\pg} DNA.",
#           "\\end{tablenotes}",
#           "\\end{threeparttable}",
#           "\\end{adjustbox}",
#           tab1[36])
# 
# write_lines(tab1, file.path("..", "article-FSIGEN", "table1.tex"))
# file.remove(file.path("..", "article-FSIGEN", "xtable1.tex"))
```


Relative to the HSG, the SMLR call rate and accuracy is increased by:
```{r}
dd_smlr_nc_wc |> 
  mutate("CR_improvement" = ((Tot_calls-SMLR_NC)/Tot_calls - (Tot_calls - HSG_NC)/Tot_calls)*100,
         "AC_improvement" = ((Tot_calls-SMLR_NC-SMLR_WC)/(Tot_calls-SMLR_NC) - (Tot_calls-HSG_NC-HSG_WC)/(Tot_calls-HSG_NC))*100) |> 
  select(Dilution, Tot_calls, SMLR_call_rate, Improve_NC, SMLR_accuracy, Improve_WC, CR_improvement, AC_improvement)
```


# Het and hom WCs for the HSG
```{r}
dd_GT_article_both <- bind_rows(dd_GT_article[,1:26], dd_GT_2nd_article)
all_WC_article <- dd_GT_article_both |> filter(Genotype != "NC", Genotype != Genotype_true)

cat("Number of WCs:\n")
all_WC_article |> nrow()
cat("Number heterozygous of WCs:\n")
all_WC_article |> filter(Genotype_true_AA == "A1A2") |> nrow()
cat("Number homozygous of WCs:\n")
all_WC_article |> filter(Genotype_true_AA != "A1A2") |> nrow()
```

# Heterozygous proportions
```{r}
N_het <- dd_GT_article |> filter(Dilution == 1000, Run == "C", Genotype_true_AA == "A1A2") |> nrow()
N_tot <- dd_GT_article |> filter(Dilution == 1000, Run == "C") |> nrow()
N_het/N_tot

N_het_val <- dd_GT_2nd_article |> filter(Dilution == 50, Genotype_true_AA == "A1A2") |> nrow()
N_tot_val <- dd_GT_2nd_article |> filter(Dilution == 50) |> nrow()
N_het_val/N_tot_val
```


## Data QC
### Noise or rare alleles?
The 165 SNPs on the Precision ID Ancestry Panel are expected to be biallelic, i.e.
for each marker, we only expect to see reads in two of the four channels.
We will of course also see some noise, but hopefully nothing that makes us doubt
whether we see a rare allele or just noise.
A rare allele could be the case, if the noise signal has a size on the level of
the signal for the allowable allele with the minimum signal when the true
genotype is heterozygous (for the homozygous genotypes, we expect the allowable
allele with the minimum signal to be comparable with noise).
```{r}
# The reads reads from the two non-expected channels will be called Noise, and
# we then compute the ratio of Noise to the number of reads for the allowed
# allele with the minimum number of reads.
non_allowable_reads <- dd_GT_both |>
  mutate(Noise = Coverage - A1.Reads - A2.Reads) |> 
  filter(Noise > 0) |> 
  mutate(Noise.to.min.allele = Noise / pmin(A1.Reads, A2.Reads)) |> 
  mutate(Noise.to.max.allele = Noise / pmax(A1.Reads, A2.Reads))
```

We will now inspect the cases where the true genotype is heterozygous and the
Noise-to-min-allele-ratio is larger than 0.5:
```{r}
nar_het <- non_allowable_reads |> 
  filter(Noise.to.min.allele >= 0.5,
         Genotype_true_AA == "A1A2")
```
This gives us 36 observations with "high noise", but all of the cases are for
the three lowest dilutions (where we do expect drop out of alleles etc.),
meaning that we didn't see any "high noise" for the dilutions with more DNA.
Hence, since the noise levels for the heterozygous genotypes looks fine for the
reasonable dilutions, there is no immediate indication, that any of our
individuals should have a heterozygous genotype with a rare allele.

We will now inspect the cases where the true genotype is homozygous, but for
these observations, we do expect that the allele with the minimum signal will
have reads close to zero, so the `Noise.to.min.allele` is a bad indicator.
Thus, in order to reject the idea that a marker with "high noise" really was a
heterozygous genotype with a rare allele, we have to rely on the majority of
times that such a potential case was measured for the individual under consideration.
```{r}
# All heterozygous and homozygous in very high dilutions
all_het <- dd_GT |> filter(Dilution %in% c(1000, 500),
                           Genotype_true_AA == "A1A2") |> arrange(Ratio)
all_hom <- dd_GT |> filter(Dilution %in% c(1000, 500),
                           Genotype_true_AA != "A1A2") |> arrange(desc(Ratio))

# Homozygous and heterozygous with "non-allowable-reads"
# (more accurately with non-expected reads)
nar_hom <- non_allowable_reads |> 
  filter(Genotype_true_AA != "A1A2",
         Noise.to.max.allele > max(all_hom$Ratio),
         pmax(A1.Reads, A2.Reads) > 100)
  
dd_rare <- nar_hom |> group_by(ID, Target.ID) |> summarise("N" = n()) |> filter(N >= 3)
nar_hom <- nar_hom |> inner_join(dd_rare, by = c("ID", "Target.ID")) |> 
  arrange(ID, Target.ID, Dilution, Run)

table(nar_hom$Target.ID)
```
Maybe the filtering-values should be chosen differently, but we do nevertheless note that rs7722456 appears quite frequently and also with Noise.to.max.allele that could rise suspicion towards the genotype being heterozygous with a rare allele.
However, since this phenomenon is only seen in the "original" dilution series, and for all five individuals in the series, it also rises suspicion towards it being an artefact of that particular dataset.
```{r}
(dd_rs7722456 <- dd_GT |> filter(Target.ID == "rs7722456") |> 
   select(ID, Run, Dilution, Genotype, QC, Genotype_true) |> 
   arrange(desc(Dilution)))
sum(dd_rs7722456$Genotype == "NC")
```
The adenine reads of rs7722456 could in principle be a rare allele with the adenine reads as the minor allele signal, so it is a problem that the HSG accepts it as a homozygous genotype without flagging it for any inspection.
The SMLR model can be used to flag such a case or make it a no-call:
```{r}
dd_GT_rare <- dd_GT_both |> filter(Target.ID == "rs7722456") |> 
  mutate(A1.Reads = A.Reads)
dd_GT_rare$Dilution <- factor(dd_GT_rare$Dilution,
                              levels = unique(dd_GT_rare$Dilution),
                              labels = paste0(unique(dd_GT_rare$Dilution), "pg"))

q <- 0.99
dd_GT_rare <- predict_prob_threshold(dd_GT_rare, m_fit, q)

b <- m_fit$par
l2_t1.max_I <- uniroot(function(x) (-b[1]-b[3]*x)/b[2]-56.5, interval = c(0, 57))$root
l1_root <- uniroot(function(x) (-b[1]-b[2]*x)/b[3], interval = c(-1, 1)*57)$root
l2_root <- uniroot(function(x) (-b[1]-b[3]*x)/b[2], interval = c(-1, 1)*57)$root
l1_l2_meet <- uniroot(function(x) (-b[1]-b[3]*x)/b[2] - (-b[1]-b[2]*x)/b[3], interval = c(-5, 5)*57)$root
dd_lines1 <- data.frame(t1 = c(l1_root+10^(-6), l1_l2_meet+10^(-6), seq(from=0, to=57, length.out=63))) %>%
  arrange(t1) %>%
  filter(t1 >= 0) %>%
  mutate(t2 = (-b[1]-b[2]*t1)/b[3], Model = "intercept")
dd_lines2 <- data.frame(t1 = c(l2_root+10^(-6), l1_l2_meet+10^(-6), seq(from=0, to=l2_t1.max_I, length.out=63))) %>%
  arrange(t1) %>%
  filter(t1 >= 0) %>%
  mutate(t2 = (-b[1]-b[3]*t1)/b[2], Model = "intercept")
if (l1_l2_meet > 0) {
  dd_lines1$t1[dd_lines1$t1 < l1_l2_meet] <- 0
  dd_lines1$t2[dd_lines1$t2 < l1_l2_meet] <- 0
  dd_lines2$t1[dd_lines2$t1 < l1_l2_meet] <- 0
  dd_lines2$t2[dd_lines2$t2 < l1_l2_meet] <- 0
}
dd_lines1 <- dd_lines1 %>% filter(t1 >= 0)
dd_lines2 <- dd_lines2 %>% filter(t2 >= 0)

dd_bands <- band_line(bind_rows(dd_GT_rare, dd_GT_rare |> mutate(A1.Reads = A2.Reads, A2.Reads = A1.Reads)), model = m_fit, q_threshold = q, f = m_fit$f) %>%
  mutate(Band = paste0("q = ", round(q, digits = 6)))

dd_missed <- dd_GT_rare %>%
  mutate(QC = if_else(is.na(QC), "", QC)) |> 
  mutate("MAF-flag" =if_else(QC == "_MAF;", "HSG", "")) |> 
  filter(`MAF-flag` == "HSG")

(pm <- ggplot(dd_GT_rare, aes(x = m_fit$f(A1.Reads), y = m_fit$f(A2.Reads))) +
  geom_polygon(aes(x=x, y=y, fill = Band), data = dd_bands) +
  scale_fill_manual(name = "Threshold",
                    values = "grey75",
                    labels = unname(c(TeX(paste0("q = ", round(q, digits = 6)))))) +
  geom_line(aes(x=t1, y=t2, linetype=Model), data = dd_lines1) +
  geom_line(aes(x=t1, y=t2, linetype=Model), data = dd_lines2) +
  scale_linetype_manual(values = "solid",
                        labels = unname(TeX("$\\beta_0 \\neq 0$"))) +
  geom_point(aes(colour = Genotype_true_AA), size = 1) +
  geom_point(aes(colour = Genotype_true_AA), data = dd_missed, size = 1) +
  scale_colour_manual(name="Rare genotype",
                      values=c("turquoise"),
                      breaks=c("A2A2"),
                      labels=unname(c(TeX("$a_1 a_2$ or $a_2 a_2$?")))) +
  # coord_cartesian(xlim=c(0,0.15),ylim=c(0,0.15))+
  xlab(TeX("$\\sqrt{s_1}$")) + ylab(TeX("$\\sqrt{s_2}$")) +
  # geoms below will use another colour scale
  new_scale_color() +
  geom_point(aes(shape = `MAF-flag`, colour = `MAF-flag`), data = dd_missed) + 
  scale_shape_manual(values = c(4,3)) +
  scale_colour_manual(name="MAF-flag",
                      values="red",
                      breaks="HSG") +
  coord_fixed(xlim = c(2,53.8), ylim = c(2,54)))

(pm2 <- pm + facet_wrap(vars(Dilution)) +
    theme(strip.text.x.top = element_text(size = plot_title_size),
        axis.title = element_text(size = axis_title_size),
        axis.text = element_text(size = axis_ticks_size),
        legend.title = element_text(size = legend_title_size),
        legend.text = element_text(size = legend_text_size),
        legend.key.size = unit(3, 'mm'),
        legend.margin = margin(t=4.5, r=0, b=4.5, l=5.5, unit = 'pt'),
        panel.background = element_rect(fill = "white"),
        panel.grid = element_line(colour = "grey92"),
        plot.margin = margin(t=0, r=0, b=0, l=0, unit='pt')))

ggsave(filename = file.path("..", "article-FSIGEN", "FIG_S3-1.5-column.bmp"),
       plot = pm2, device = "bmp", width = 5512, height = 3530, units = "px", dpi = 1000)

```

## Number of MAF flags for rs7722456
```{r}
dd_GT_rare |> nrow()
6*5*2 + 5*5 + 2 + 4*5 + 1
dd_GT_rare |> select(QC) |> filter(!is.na(QC)) |> filter(QC == "_MAF;") |> nrow()
sum(dd_missed$`MAF-flag` == "HSG")
sum(dd_GT_rare$Genotype_pred == "NC")

QC <- dd_GT_rare |> group_by(Dilution) |> summarise("Total" = n())
QC <- dd_GT_rare |> filter(!is.na(QC)) |> group_by(Dilution) |> summarise("MAF-flags" = sum(QC == "_MAF;")) |> inner_join(QC, by = "Dilution")
QC$MAF_pct <- round(100*QC$`MAF-flags`/QC$Total, digits = 3)
QC
```



# Are any SNP positions more prone to become WCs or NCs than others?
```{r}
dd_WC <- dd_GT_both |> filter(Genotype != Genotype_true & Genotype != "NC")
dd_NC <- dd_GT_both |> filter(Genotype != "NC")

tbl_WC <- table(dd_WC$Target.ID)
tbl_NC <- table(dd_NC$Target.ID)
# tbl_WC <- tbl_WC[order(as.integer(substring(names(tbl_WC), 3)))]
# tbl_NC <- tbl_NC[order(as.integer(substring(names(tbl_NC), 3)))]

plot(1:length(tbl_WC), unname(tbl_WC))
plot(1:length(tbl_NC), unname(tbl_NC))
tbl_WC[order(tbl_WC, decreasing = T)]
tbl_NC[order(tbl_NC, decreasing = T)]
# names(tbl_WC)[order(as.integer(substring(names(tbl_WC), 3)))]
```



# Deviation of slope and intercept for cases with and without complete separation
```{r}
df_boot_NI50 <- readRDS(file.path("..", "data", "01_obj_bootstrap_sqrt_1K.rds")) |> 
  filter(Dilution == "50pg")
df_boot_NI50_CS <- df_boot_NI50 |> filter(n_WC == 0)
df_boot_NI50 <- df_boot_NI50 |> filter(n_WC > 0)

df_boot50 <- readRDS(file.path("..", "data", "01_obj_bootstrap_sqrt_1K.rds")) |> 
  filter(Dilution == "50pg")
df_boot50_CS <- df_boot50 |> filter(n_WC == 0)
df_boot50 <- df_boot50 |> filter(n_WC > 0)
```

## Standard deviation of model with no intercept
```{r}
cat("Complete separation (slope):\n")
var(df_boot_NI50_CS$beta1/df_boot_NI50_CS$beta2)
cat("Overlapping data (slope):\n")
var(df_boot_NI50$beta1/df_boot_NI50$beta2)

cat("Complete separation (slope):\n")
var(df_boot_NI50_CS$beta2/df_boot_NI50_CS$beta1)
cat("Overlapping data (slope):\n")
var(df_boot_NI50$beta2/df_boot_NI50$beta1)
```

## Standard deviation of intercept-model
```{r}
cat("Complete separation (intercept):\n")
var(df_boot50_CS$beta0/df_boot50_CS$beta1)
cat("Overlapping data (intercept):\n")
var(df_boot50$beta0/df_boot50$beta1)

cat("Complete separation (intercept):\n")
var(df_boot50_CS$beta0/df_boot50_CS$beta2)
cat("Overlapping data (intercept):\n")
var(df_boot50$beta0/df_boot50$beta2)

cat("Complete separation (slope):\n")
var(df_boot50_CS$beta1/df_boot50_CS$beta2)
cat("Overlapping data (slope):\n")
var(df_boot50$beta1/df_boot50$beta2)

cat("Complete separation (slope):\n")
var(df_boot50_CS$beta2/df_boot50_CS$beta1)
cat("Overlapping data (slope):\n")
var(df_boot50$beta2/df_boot50$beta1)
```


# Range of reduction in NCs
```{r}
df_red_HSG_low <- readRDS(file.path("..", "data", "02-obj-NNred-HSG-sqrt_NI-low.rds"))[-c(9, 11)]
df_red_HSG_high <- readRDS(file.path("..", "data", "02-obj-NNred-HSG-sqrt_NI-high.rds"))[-c(9, 11)]

cat("HSG (low):\n")
quantile(as.matrix(df_red_HSG_low), probs = c(0.25, 0.5, 0.75))

cat("HSG (high):\n")
quantile(as.matrix(df_red_HSG_high), probs = c(0.25, 0.5, 0.75))
```

```{r}
cat("HSG low:\n")
for (i in df_red_HSG_low) {
  print(quantile(i, probs = c(0.25, 0.5, 0.75)))
}

cat("HSG high:\n")
cat("100% for all quantiles in all cross-validations!\n")
# for (i in df_red_HSG_high) {
#   print(quantile(i, probs = c(0.25, 0.5, 0.75)))
# }
```

```{r}
red_HSG_high <- as.vector(as.matrix(df_red_HSG_high))
red_HSG_high <- red_HSG_high[order(red_HSG_high)]
sum(red_HSG_high < 1) / length(red_HSG_high)
```



# Empirical assessment of conditional genotype probabilities
Up to symmetry, each unique pair of signals makes its own conditional genotype probability function [the symmetry means that (s1,s2)=(30,20) mirrors (20,30)].
Despite the symmetry, there are very many unique signal coordinates, and even for each of the non-unique coordinates, we most often (always) find that all observations with such a coordinate has a correct genotype prediction.
In other words: all observations with a wrong genotype prediction have unique signal coordinates.
This means that the empirical distribution at each observed signal coordinate either has 100% correctly or 100% wrongly classified genotypes.
Thus, empirical verification can't be done in a strictly correct manner, but needs to be done by pooling coordinates together and in some way assess their prediction distributions together (median, mean, range).

First we pool coordinates by creating groups in a polar coordinate system.
We seek to define pooling areas in this coordinate system, such that each group approximately has an equal number of observations.
Then we conduct a binomial test for each group to see whether the proportion of correctly classified observations can be explained by the mean probability for the predicted genotype in that group, i.e. by the mean of the model's maximal genotype probability for the observations' coordinates.
```{r}
# Mirror A1.Reads and A2.Reads, such that (c1, c2) lies below the identity line
dd_fit_low <- dd_GT_article_both |>
  filter(Dilution %in% c(50, 31.25)) |>
  mutate(c1 = if_else(A1.Reads >= A2.Reads, A1.Reads, A2.Reads),
         c2 = if_else(A2.Reads < A1.Reads, A2.Reads, A1.Reads),
         radius = sqrt(c1^2 + c2^2),
         phi = atan2(c2,c1))
m_fit0 <- fit_symmetry_model2(dd_fit_low, f = sqrt, intercept = F, b_int = c(1,-2))
dd_fit_low <- predict_prob_threshold(dd_fit_low, m_fit0) |> 
  mutate(zyg_p = if_else(Genotype_pred_AA == "A1A2", "het", "hom")) |> 
  select(Dilution, c1, c2, radius, phi, Genotype_true, Genotype_pred, zyg_p, p_max)

# The homozygous and heterozygous probability functions are assessed separately
for (i in c(seq(20,100,5), 150)) {
dd_hom0 <- dd_fit_low |> filter(zyg_p == "hom", c2 == 0)
dd_hom <- dd_fit_low |> filter(zyg_p == "hom", c2 > 0)
dd_het <- dd_fit_low |> filter(zyg_p == "het")

# Cut points for equally sized radial groups
gr_size <- i
hom0_cut <- quantile(dd_hom0$c1, seq(0, 1, length.out = ceiling(nrow(dd_hom0)/gr_size))[-1]) |> floor() |> unname()
hom_cut <- quantile(dd_hom$radius, seq(0, 1, length.out = ceiling(nrow(dd_hom)/gr_size))[-1]) |> unname()
het_cut <- quantile(dd_het$radius, seq(0, 1, length.out = ceiling(nrow(dd_het)/(2*gr_size)))[-1]) |> unname()

dd_hom0$rg <- cut(dd_hom0$c1,
                  breaks = c(0, hom0_cut, Inf),
                  labels = FALSE,
                  include.lowest = TRUE)
dd_hom$rg <- cut(dd_hom$radius,
                 breaks = c(0, hom_cut, Inf),
                 labels = FALSE,
                 include.lowest = TRUE)
dd_het$rg <- cut(dd_het$radius,
                 breaks = c(0, het_cut, Inf),
                 labels = FALSE,
                 include.lowest = TRUE)
dd_het <- dd_het |> group_by(rg) |> 
  mutate(pg = if_else(phi <= median(phi), 1, 2),
         gr = paste0("(", rg, ", ", pg, ")")) |> ungroup()

# Computing p-values for coordinate groups.
(p_hom0 <- dd_hom0 |> group_by(rg) |> 
    mutate(p_pred = mean(p_max),
           n_obs = n(),
           n_wc = sum(Genotype_pred != Genotype_true),
           n_correct = sum(Genotype_pred == Genotype_true),
           p_empirical = n_correct/n_obs) |> 
    distinct(c1, c2, .keep_all = TRUE) |> 
    summarise(p_pred = unique(p_pred),
              n_obs = unique(n_obs),
              n_correct = unique(n_correct),
              p_h0 = binom.test(n_correct, n_obs, p_pred, alternative = "two.sided")$p.value))

(p_hom <- dd_hom |> group_by(rg) |> 
    mutate(p_pred = mean(p_max),
           n_obs = n(),
           n_wc = sum(Genotype_pred != Genotype_true),
           n_correct = sum(Genotype_pred == Genotype_true),
           p_empirical = n_correct/n_obs) |> 
    distinct(c1, c2, .keep_all = TRUE) |> 
    summarise(p_pred = unique(p_pred),
              n_obs = unique(n_obs),
              n_correct = unique(n_correct),
              p_h0 = binom.test(n_correct, n_obs, p_pred, alternative = "two.sided")$p.value))

(p_het <- dd_het |> group_by(rg) |> 
    mutate(p_pred = mean(p_max),
           n_obs = n(),
           n_wc = sum(Genotype_pred != Genotype_true),
           n_correct = sum(Genotype_pred == Genotype_true),
           p_empirical = n_correct/n_obs) |> 
    distinct(c1, c2, .keep_all = TRUE) |> 
    summarise(p_pred = unique(p_pred),
              n_obs = unique(n_obs),
              n_correct = unique(n_correct),
              p_h0 = binom.test(n_correct, n_obs, p_pred, alternative = "two.sided")$p.value))

print(paste0("N: ", i, ";    ",
              "hom0: ", sum(p_hom0$p_h0<0.05),"/",nrow(p_hom0), ";    ",
              "hom: ", sum(p_hom$p_h0<0.05),"/",nrow(p_hom), ";    ",
              "het: ", sum(p_het$p_h0<0.05),"/",nrow(p_het)))
}
```

```{r}
# Mirror A1.Reads and A2.Reads, such that (c1, c2) lies below the identity line
dd_fit_low <- dd_GT_article_both |>
  filter(Dilution %in% c(50, 31.25)) |>
  mutate(c1 = if_else(A1.Reads >= A2.Reads, A1.Reads, A2.Reads),
         c2 = if_else(A2.Reads < A1.Reads, A2.Reads, A1.Reads))
m_fit0 <- fit_symmetry_model2(dd_fit_low, f = sqrt, intercept = F, b_int = c(1,-2))
dd_fit_low <- predict_prob_threshold(dd_fit_low, m_fit0) |> 
  mutate(zyg_p = if_else(Genotype_pred_AA == "A1A2", "het", "hom")) |> 
  select(Dilution, c1, c2, Genotype_true, Genotype_pred, zyg_p, p_max)

# The homozygous and heterozygous probability functions are assessed separately
gr_size <- c(seq(10,200,10))
for (i in gr_size) {
  dd_hom0 <- dd_fit_low |> filter(zyg_p == "hom", c2 == 0)
  dd_hom <- dd_fit_low |> filter(zyg_p == "hom", c2 > 0)
  dd_het <- dd_fit_low |> filter(zyg_p == "het")
  
  # Cut points for equally sized radial groups
  hom0_cut <- quantile(dd_hom0$p_max, seq(0, 1, length.out = ceiling(nrow(dd_hom0)/i))[-1]) |> unname() |> unique()
  hom_cut <- quantile(dd_hom$p_max, seq(0, 1, length.out = ceiling(nrow(dd_hom)/i))[-1]) |> unname() |> unique()
  het_cut <- quantile(dd_het$p_max, seq(0, 1, length.out = ceiling(nrow(dd_het)/i))[-1]) |> unname() |> unique()
  
  dd_hom0$prob_gr <- cut(dd_hom0$p_max,
                         breaks = c(0, hom0_cut, Inf),
                         labels = FALSE,
                         include.lowest = TRUE)
  dd_hom$prob_gr <- cut(dd_hom$p_max,
                        breaks = c(0, hom_cut, Inf),
                        labels = FALSE,
                        include.lowest = TRUE)
  dd_het$prob_gr <- cut(dd_het$p_max,
                        breaks = c(0, het_cut, Inf),
                        labels = FALSE,
                        include.lowest = TRUE)
  
  # Computing p-values for coordinate groups.
  (p_hom0 <- dd_hom0 |> group_by(prob_gr) |> 
    mutate(p_pred = mean(p_max),
           n_obs = n(),
           n_wc = sum(Genotype_pred != Genotype_true),
           n_correct = sum(Genotype_pred == Genotype_true),
           p_empirical = n_correct/n_obs) |> 
    distinct(c1, c2, .keep_all = TRUE) |> 
    summarise(p_pred = unique(p_pred),
              n_obs = unique(n_obs),
              n_correct = unique(n_correct),
              p_h0 = binom.test(n_correct, n_obs, p_pred, alternative = "two.sided")$p.value))
  
  (p_hom <- dd_hom |> group_by(prob_gr) |> 
    mutate(p_pred = mean(p_max),
           n_obs = n(),
           n_wc = sum(Genotype_pred != Genotype_true),
           n_correct = sum(Genotype_pred == Genotype_true),
           p_empirical = n_correct/n_obs) |> 
    distinct(c1, c2, .keep_all = TRUE) |> 
    summarise(p_pred = unique(p_pred),
              n_obs = unique(n_obs),
              n_correct = unique(n_correct),
              p_h0 = binom.test(n_correct, n_obs, p_pred, alternative = "two.sided")$p.value))
  
  (p_het <- dd_het |> group_by(prob_gr) |> 
    mutate(p_pred = mean(p_max),
           n_obs = n(),
           n_wc = sum(Genotype_pred != Genotype_true),
           n_correct = sum(Genotype_pred == Genotype_true),
           p_empirical = n_correct/n_obs) |> 
    distinct(c1, c2, .keep_all = TRUE) |> 
    summarise(p_pred = unique(p_pred),
              n_obs = unique(n_obs),
              n_correct = unique(n_correct),
              p_h0 = binom.test(n_correct, n_obs, p_pred, alternative = "two.sided")$p.value))
  
  if (i == gr_size[1]) {print("Group size   #p < 0.05 / #Tests (i.e. #Groups)")}
  print(paste0("N = ", i, ";      ",
               "hom0: ", sum(p_hom0$p_h0<0.05),"/",nrow(p_hom0), ";    ",
               "hom: ", sum(p_hom$p_h0<0.05),"/",nrow(p_hom), ";      ",
               "het: ", sum(p_het$p_h0<0.05),"/",nrow(p_het)))
}
```



# Supplementary
## Signals are very similar across the four nucleotides
```{r}
dd_nucleotide <- dd_GT_article_both |> 
  pivot_longer(cols = c(A.Reads, C.Reads, G.Reads, T.Reads),
               names_to = "Nucleotide", values_to = "Signal") |> 
  mutate(Nucleotide = str_sub(Nucleotide, 1, 1))

dd_noise <- dd_nucleotide |>
  # filter(Nucleotide == A1 | Nucleotide == A2) |> 
  filter(Nucleotide != str_sub(Genotype_true,1,1) & Nucleotide != str_sub(Genotype_true,2,2)) |> 
  filter(Signal > 0) # By definition, a zero signal is not noise

dd_nucleotide <- dd_nucleotide |>
  filter(Nucleotide == str_sub(Genotype_true,1,1) | Nucleotide == str_sub(Genotype_true,2,2))


p_s <- dd_nucleotide |> filter(Dilution %in% c(50, 31.25)) |> 
  ggplot(aes(x = sqrt(Signal), colour = Nucleotide)) +
  stat_density(aes(colour = Nucleotide), geom = "line", position = "identity",
               trim = T, bounds = c(0, Inf), adjust = 2.5) +
  labs(x = TeX("$\\sqrt{s}$"), y = "Density", title = "50pg & 31.25pg", subtitle = "Genotype signal") +
  theme_bw()

p_no <- dd_noise |> filter(Dilution %in% c(50, 31.25)) |> 
  ggplot(aes(x = sqrt(Signal), colour = Nucleotide)) +
  stat_density(aes(colour = Nucleotide), geom = "line", position = "identity",
               trim = T, bounds = c(1, Inf), adjust = 4.5) +
  labs(x = TeX("$\\sqrt{s}$"), y = NULL, subtitle = "Noise") +
  theme_bw()

p <- p_s + p_no + plot_layout(ncol = 2, nrow = 1, guides = "collect") & 
  theme(plot.title = element_text(size = plot_title_size),
        plot.subtitle = element_text(size = plot_title_size),
        axis.title = element_text(size = axis_title_size),
        axis.text = element_text(size = axis_ticks_size),
        legend.title = element_text(size = legend_title_size),
        legend.text = element_text(size = legend_text_size))

# Leftmost plot should have its left margin set to zero
p[[1]]$theme$plot.margin[4] <- 0*p[[1]]$theme$plot.margin[4]
# Rightmost plot should have its right margin set to zero
p[[2]]$theme$plot.margin[2] <- 0*p[[2]]$theme$plot.margin[2]
# Both plots should have their top and bottom margins set to zero
p[[1]]$theme$plot.margin[c(1,3)] <- 0*p[[1]]$theme$plot.margin[c(1,3)]
p[[2]]$theme$plot.margin[c(1,3)] <- 0*p[[2]]$theme$plot.margin[c(1,3)]
# The legends' right margins should be set to zero
p[[1]]$theme$legend.margin[2] <- 0*p[[1]]$theme$legend.margin[2]
p[[2]]$theme$legend.margin[2] <- 0*p[[2]]$theme$legend.margin[2]
# And the overall margins added by patchwork should be set to zero:
(p <- p + plot_annotation(theme = theme(plot.margin = margin(t=0, r=0, b=0, l=0, unit = 'pt'))))

ggsave(filename = file.path("..", "article-FSIGEN", "FIG_S4-two-column.bmp"),
       plot = p, device = "bmp", width = 7480, height = 3425, units = "px", dpi = 1000)
```


# Proportionality
The following shows that the no-call alignment is almost always exact.
This implies that the proportionality is almost always exact.
For the few non-exact cases, it is also shown that the difference in aligned no-calls is very small.
This implies that the proportionality is then approximate.
```{r}
m <- c("sqrt_NI-C", "sqrt-C", "log_NI-C", "log-C", "I_NI-C", "I-C")
ff <- c("sqrt_NI_", "sqrt_", "log_NI_", "log_", "I_NI_", "I_")
datafile_name <- paste0("01_obj_crossval_", ff, "1K_9individuals_",
                        rep(c("low", "high"), each=length(ff)), ".rds")

# Difference in NCs when SMLR's NCs are aligned to those of the HSG
NC_diff_smlr_hsg <- lapply(datafile_name, function(x) {
  dd_val_dils <- readRDS(file.path("..", "data", x))
  names(dd_val_dils) <- LETTERS[1:length(dd_val_dils)]
  # Extracting NC differences for the six cross-validations in the supplementary material
  dd_val_dils[c("D", "B", "J")] |>
    sapply(function(d){
      sapply(d, function(r){
        hsg_nc <- r$HSG_NN
        p_all <- c(r$p_WC, r$p_NN)
        if (length(p_all) > hsg_nc) {
          p_all <- p_all[order(p_all)]
          # NOTE: p_all is listed from low to high, so entry hsg_nc is the last
          # point we need to turn into an NC, before SMLR makes at least as many
          # no-calls has HSG. The question is whether the p at this point is
          # repeated at the next point, such that more points are turned into NCs
          # at that q-threshold.
          q <- p_all[hsg_nc]
          smlr_nc <- max(which(p_all == q))
        } else {
          smlr_nc <- hsg_nc
        }
        return(smlr_nc - hsg_nc)
      })
    })
})
names(NC_diff_smlr_hsg) <- gsub("1K_9individuals|\\.rds", "", str_sub(datafile_name, 19))
names(NC_diff_smlr_hsg) <- sub("__", "_", names(NC_diff_smlr_hsg))

lapply(NC_diff_smlr_hsg, function(cval) {
  apply(cval, MARGIN=2, function(x) {sum(x == 0)})
}) |> bind_rows(.id = "CV")

pooled_NC_diffs <- NC_diff_smlr_hsg |> unlist() |> unname()
# Proportion of exact alignments for all cross-validations pooled together
sum(pooled_NC_diffs == 0) / length(pooled_NC_diffs)

# Maximal NC difference after NC-alignment across all cross-validations
(max_diff <- max(pooled_NC_diffs))

# Size range of test data
n_at_max <- lapply(datafile_name, function(x) {
  dd_val_dils <- readRDS(file.path("..", "data", x))
  names(dd_val_dils) <- LETTERS[1:length(dd_val_dils)]
  dd_val_dils[c("D", "B", "J")] |>
    sapply(function(d){
      sapply(d, function(r){
        hsg_nc <- r$HSG_NN
        p_all <- c(r$p_WC, r$p_NN)
        if (length(p_all) > hsg_nc) {
          p_all <- p_all[order(p_all)]
          q <- p_all[hsg_nc]
          smlr_nc <- max(which(p_all == q))
        } else {
          smlr_nc <- hsg_nc
        }
        if (smlr_nc - hsg_nc == max_diff) {
          return(r$N_test)
        } else {
          return(NULL)
        }
      })
    })
}) |> unlist() |> unname()
n_at_max / (n_at_max - max_diff)

# High percentiles
quantile(pooled_NC_diffs, probs = c(0.75000, 0.77635,
                                    0.77639, 0.85996,
                                    0.86000, 0.90894,
                                    0.90897, 0.94119,
                                    0.94123, 0.96180,
                                    0.96184, 0.97766,
                                    0.97770, 0.98911,
                                    0.98914, 0.99536,
                                    0.99539, 0.99863,
                                    0.99867, 0.99972,
                                    0.99975, 0.99991,
                                    0.99995, 0.99997,
                                    0.9999999999))
quantile(pooled_NC_diffs[pooled_NC_diffs > 0], probs = c(0.25, 0.37,
                                                         0.38, 0.59,
                                                         0.60, 0.73,
                                                         0.74, 0.82,
                                                         0.83, 0.90,
                                                         0.91, 0.95,
                                                         0.96, 0.97,
                                                         0.98, 0.993,
                                                         0.995, 0.998,
                                                         0.999, 0.9996,
                                                         0.9998, 0.9999999994))

# Computing all observed (TC - NC_{\text{HSG}} / (TC - NC_{\text{SMLR}}(q_{NC}))
TC_NC_ratio <- lapply(datafile_name, function(x) {
  dd_val_dils <- readRDS(file.path("..", "data", x))
  names(dd_val_dils) <- LETTERS[1:length(dd_val_dils)]
  dd_val_dils[c("D", "B", "J")] |>
    sapply(function(d){
      sapply(d, function(r){
        hsg_nc <- r$HSG_NN
        p_all <- c(r$p_WC, r$p_NN)
        if (length(p_all) > hsg_nc) {
          p_all <- p_all[order(p_all)]
          q <- p_all[hsg_nc]
          smlr_nc <- max(which(p_all == q))
        } else {
          smlr_nc <- hsg_nc
        }
        return((r$N_test - hsg_nc) / (r$N_test - smlr_nc))
      })
    })
}) |> unlist() |> unname()

quantile(TC_NC_ratio, probs = c(0.7763, 0.8782, 0.9514, 0.9850, 0.9963, 0.9996, 0.99998, 1))
quantile(TC_NC_ratio[TC_NC_ratio > 1], probs = c(0.25, 0.5, 0.75, 0.8, 0.9, 0.95, 0.98, 0.99))
```